{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETFound: Fine-Tuning and Evaluation for Diabetic Retinopathy\n",
    "\n",
    "This notebook provides a complete, end-to-end workflow to reproduce the fine-tuning of the RETFound model on the IDRiD dataset for diabetic retinopathy classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# ## 1. Environment Setup\n",
    "#\n",
    "# This section clones the original RETFound_MAE repository, installs all\n",
    "# necessary dependencies, and applies essential patches to the scripts to ensure\n",
    "# they run correctly in the Colab environment.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Clone Repository & Set Working Directory ---\n",
    "repo_dir = 'RETFound_MAE'\n",
    "\n",
    "if not os.path.isdir(repo_dir):\n",
    "  print(\"Cloning repository...\")\n",
    "  !git clone https://github.com/rmaphoh/RETFound_MAE/\n",
    "else:\n",
    "  print(\"Repository already exists. Skipping clone.\")\n",
    "\n",
    "%cd {repo_dir}\n",
    "\n",
    "# --- Install Dependencies ---\n",
    "print(\"\\n‚è≥ Installing dependencies...\")\n",
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121 -q\n",
    "!pip install timm==0.9.16 pandas==2.2.2 scikit-learn -q\n",
    "print(\"‚úÖ Dependencies installed.\")\n",
    "\n",
    "# --- Apply Initial Patches ---\n",
    "# Note: Further patches are applied programmatically in the next section.\n",
    "print(\"\\n‚öôÔ∏è Patching script to bypass errors...\")\n",
    "!sed -i \"s/if True:  # args.distributed:/if args.distributed:/g\" main_finetune.py\n",
    "print(\"‚úÖ Script patched.\")\n",
    "\n",
    "print(\"\\nüéâ Setup complete. Ready for the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# ## 2. Generate Core Scripts\n",
    "#\n",
    "# We programmatically overwrite `main_finetune.py` and `engine_finetune.py`\n",
    "# to incorporate all the fixes and improvements we developed. This includes\n",
    "# robust metric calculation (AUROC, Specificity), error handling, and disabling\n",
    "# conflicting libraries.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create main_finetune.py ---\n",
    "main_finetune_content = r\"\"\"\n",
    "import argparse, datetime, json, numpy as np, os, time\n",
    "from pathlib import Path\n",
    "import torch, torch.backends.cudnn as cudnn\n",
    "from timm.data.mixup import Mixup\n",
    "import models_vit as models, util.lr_decay as lrd, util.misc as misc\n",
    "from util.datasets import build_dataset\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from huggingface_hub import hf_hub_download\n",
    "from engine_finetune import train_one_epoch, evaluate\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('MAE fine-tuning for image classification', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=128, type=int)\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "    parser.add_argument('--accum_iter', default=1, type=int)\n",
    "    parser.add_argument('--model', default='vit_large_patch16', type=str)\n",
    "    parser.add_argument('--input_size', default=256, type=int)\n",
    "    parser.add_argument('--drop_path', type=float, default=0.2)\n",
    "    parser.add_argument('--clip_grad', type=float, default=None)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.05)\n",
    "    parser.add_argument('--lr', type=float, default=None)\n",
    "    parser.add_argument('--blr', type=float, default=5e-3)\n",
    "    parser.add_argument('--layer_decay', type=float, default=0.65)\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-6)\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=10)\n",
    "    parser.add_argument('--finetune', default='', type=str)\n",
    "    parser.add_argument('--task', default='', type=str)\n",
    "    parser.add_argument('--global_pool', action='store_true', default=True)\n",
    "    parser.add_argument('--data_path', default='./data/', type=str)\n",
    "    parser.add_argument('--nb_classes', default=5, type=int)\n",
    "    parser.add_argument('--output_dir', default='./output_dir')\n",
    "    parser.add_argument('--log_dir', default='./output_logs')\n",
    "    parser.add_argument('--device', default='cuda')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='')\n",
    "    parser.add_argument('--start_epoch', default=0, type=int)\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=2, type=int)\n",
    "    parser.add_argument('--pin_mem', action='store_true', default=True)\n",
    "    parser.add_argument('--world_size', default=1, type=int)\n",
    "    parser.add_argument('--local_rank', default=-1, type=int)\n",
    "    parser.add_argument('--dist_on_itp', action='store_true')\n",
    "    parser.add_argument('--dist_url', default='env://')\n",
    "    return parser\n",
    "\n",
    "def main(args):\n",
    "    misc.init_distributed_mode(args)\n",
    "    device = torch.device(args.device)\n",
    "    seed = args.seed + misc.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    dataset_test = build_dataset(is_train='test', args=args)\n",
    "    if not args.eval:\n",
    "        dataset_train = build_dataset(is_train='train', args=args)\n",
    "        dataset_val = build_dataset(is_train='val', args=args)\n",
    "    else:\n",
    "        dataset_train, dataset_val = None, None\n",
    "\n",
    "    if args.distributed:\n",
    "        # ... (distributed setup omitted for Colab clarity)\n",
    "        pass\n",
    "    else:\n",
    "        sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "        if not args.eval:\n",
    "            sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "            sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    log_writer = None # Disabled for Colab\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(dataset_test, sampler=sampler_test, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=args.pin_mem, drop_last=False)\n",
    "    if not args.eval:\n",
    "        data_loader_train = torch.utils.data.DataLoader(dataset_train, sampler=sampler_train, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=args.pin_mem, drop_last=True)\n",
    "        data_loader_val = torch.utils.data.DataLoader(dataset_val, sampler=sampler_val, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=args.pin_mem, drop_last=False)\n",
    "\n",
    "    model = models.__dict__[args.model](img_size=args.input_size, num_classes=args.nb_classes, drop_path_rate=args.drop_path, global_pool=args.global_pool)\n",
    "\n",
    "    if args.finetune and not args.eval:\n",
    "        print(f\"Downloading pre-trained weights from Hugging Face: {args.finetune}\")\n",
    "        checkpoint_path = hf_hub_download(repo_id=f'YukunZhou/{args.finetune}', filename=f'{args.finetune}.pth')\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')['model']\n",
    "        msg = model.load_state_dict(checkpoint, strict=False)\n",
    "        print(f\"Loaded pre-trained checkpoint from {args.finetune} with message: {msg}\")\n",
    "\n",
    "    if args.resume:\n",
    "        print(f\"Resuming from checkpoint: {args.resume}\")\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')['model']\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    model.to(device)\n",
    "    print(f'Number of model params (M): {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1.e6:.2f}')\n",
    "\n",
    "    if args.eval:\n",
    "        evaluate(data_loader_test, model, device, args, 0, 'test', args.nb_classes, log_writer)\n",
    "        return\n",
    "\n",
    "    eff_batch_size = args.batch_size * misc.get_world_size()\n",
    "    if args.lr is None: args.lr = args.blr * eff_batch_size / 256\n",
    "    print(f\"Actual lr: {args.lr:.2e}\")\n",
    "    param_groups = lrd.param_groups_lrd(model, args.weight_decay, no_weight_decay_list=model.no_weight_decay(), layer_decay=args.layer_decay)\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=args.lr)\n",
    "    loss_scaler = NativeScaler()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"--- Starting Training for {args.epochs} epochs ---\")\n",
    "    max_accuracy = 0.0\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        train_stats = train_one_epoch(model, criterion, data_loader_train, optimizer, device, epoch, loss_scaler, args=args)\n",
    "        val_stats, _ = evaluate(data_loader_val, model, device, args, epoch, 'val', args.nb_classes, log_writer=log_writer)\n",
    "        print(f\"EPOCH:{epoch} | Val Acc: {val_stats['acc1']:.1f}%\")\n",
    "        if max_accuracy < val_stats[\"acc1\"]:\n",
    "            max_accuracy = val_stats[\"acc1\"]\n",
    "            misc.save_model(args=args, model=model, model_without_ddp=model, optimizer=optimizer, loss_scaler=loss_scaler, epoch=epoch, mode='best')\n",
    "        print(f'Max accuracy: {max_accuracy:.2f}%')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args_parser().parse_args()\n",
    "    if args.output_dir: Path(os.path.join(args.output_dir, args.task)).mkdir(parents=True, exist_ok=True)\n",
    "    main(args)\n",
    "\"\"\"\n",
    "with open(\"main_finetune.py\", \"w\") as f:\n",
    "    f.write(main_finetune_content)\n",
    "print(\"‚úÖ `main_finetune.py` generated.\")\n",
    "\n",
    "\n",
    "# --- Create engine_finetune.py ---\n",
    "engine_finetune_content = r\"\"\"\n",
    "import math, sys\n",
    "from typing import Iterable, Optional\n",
    "import torch, torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from timm.utils import accuracy\n",
    "import util.misc as misc, util.lr_sched as lr_sched\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler, args=None):\n",
    "    model.train(True)\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = f'Epoch: [{epoch}]'\n",
    "    for data_iter_step, (samples, targets) in enumerate(metric_logger.log_every(data_loader, 20, header)):\n",
    "        lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
    "        samples, targets = samples.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = criterion(model(samples), targets)\n",
    "        loss_value = loss.item()\n",
    "        if not math.isfinite(loss_value): sys.exit(f\"Loss is {loss_value}, stopping training\")\n",
    "        loss_scaler(loss, optimizer, parameters=model.parameters())\n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.synchronize()\n",
    "        metric_logger.update(loss=loss_value)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(data_loader, model, device, args, epoch, mode, num_class, log_writer=None):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    header, model.eval(), all_preds, all_labels, all_probs = 'Test:', [], [], []\n",
    "    for batch in metric_logger.log_every(data_loader, 10, header):\n",
    "        images, target = batch[0].to(device, non_blocking=True), batch[-1].to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "        preds, probs = torch.argmax(output, dim=1), F.softmax(output, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().detach().numpy())\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.meters['acc1'].update(accuracy(output, target, topk=(1,))[0].item(), n=images.shape[0])\n",
    "    print(f'* Acc@1 {metric_logger.acc1.global_avg:.3f} loss {metric_logger.loss.global_avg:.3f}')\n",
    "    all_labels, all_preds, all_probs = np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "    print(\"\\n--- Performance Metrics ---\")\n",
    "    if len(np.unique(all_labels)) > 1 and len(np.unique(all_preds)) > 1:\n",
    "        try:\n",
    "            print(f\"AUROC (Macro): {roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro'):.4f}\")\n",
    "        except Exception as e: print(f\"Could not calculate AUROC: {e}\")\n",
    "    else: print(\"Skipping AUROC: not enough classes in labels or predictions.\")\n",
    "    cm, fp = confusion_matrix(all_labels, all_preds), cm.sum(axis=0) - np.diag(cm)\n",
    "    tn = cm.sum() - (fp + (cm.sum(axis=1) - np.diag(cm)) + np.diag(cm))\n",
    "    print(f\"Specificity (Macro): {np.mean(tn / (tn + fp)):.4f}\")\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[f'Class {i}' for i in range(num_class)], digits=4, zero_division=0))\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}, 'results'\n",
    "\"\"\"\n",
    "with open(\"engine_finetune.py\", \"w\") as f:\n",
    "    f.write(engine_finetune_content)\n",
    "print(\"‚úÖ `engine_finetune.py` generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# ## 3. Data Preparation\n",
    "#\n",
    "# Download and unzip the IDRiD dataset for 5-class DR grading.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = '../IDRiD_data'\n",
    "\n",
    "if not os.path.isdir(data_folder_path):\n",
    "    print(\"‚è≥ Dataset folder not found. Downloading and unzipping...\")\n",
    "    !gdown --id 1c6zexA705z-ANEBNXJOBsk6uCvRnzmr3 -O ../IDRiD_data.zip\n",
    "    !unzip -q -o ../IDRiD_data.zip -d ../\n",
    "    print(\"‚úÖ Dataset downloaded and ready.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Dataset already exists at '{data_folder_path}'. Skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# ## 4. Workflow A: Fine-Tune and Evaluate\n",
    "#\n",
    "# This is the primary workflow. It involves authenticating with Hugging Face,\n",
    "# running the fine-tuning process on the IDRiD dataset, and then evaluating\n",
    "# the best model produced during that run.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.1: Authenticate with Hugging Face ---\n",
    "from huggingface_hub import notebook_login\n",
    "print(\"üöÄ Please log in to Hugging Face to download the base model.\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.2: Run Fine-Tuning ---\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from main_finetune import get_args_parser, main as finetune_main\n",
    "\n",
    "print(\"\\nüöÄ Starting fine-tuning...\")\n",
    "ft_parser = get_args_parser()\n",
    "args_ft = ft_parser.parse_args([\n",
    "    '--model', 'RETFound_mae',\n",
    "    '--epochs', '20',\n",
    "    '--blr', '5e-3',\n",
    "    '--data_path', '../IDRiD_data',\n",
    "    '--task', 'RETFound_finetune_IDRiD',\n",
    "    '--output_dir', './output_dir',\n",
    "    '--log_dir', './log_dir',\n",
    "    '--finetune', 'RETFound_mae_meh'\n",
    "])\n",
    "Path(os.path.join(args_ft.output_dir, args_ft.task)).mkdir(parents=True, exist_ok=True)\n",
    "finetune_main(args_ft)\n",
    "print(\"\\n‚úÖ Fine-tuning finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.3: Evaluate the Fine-Tuned Model ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nüöÄ Starting inference on the BEST model from fine-tuning...\")\n",
    "\n",
    "best_model_path = f'./output_dir/{args_ft.task}/checkpoint-best.pth'\n",
    "\n",
    "if not os.path.exists(best_model_path):\n",
    "    print(f\"‚ùå ERROR: The expected best model was not found at {best_model_path}\")\n",
    "else:\n",
    "    print(f\"Found best model at: {best_model_path}\")\n",
    "    eval_parser = get_args_parser()\n",
    "    args_eval_ft = eval_parser.parse_args([\n",
    "        '--model', 'RETFound_mae',\n",
    "        '--eval',\n",
    "        '--data_path', '../IDRiD_data',\n",
    "        '--resume', best_model_path\n",
    "    ])\n",
    "    finetune_main(args_eval_ft)\n",
    "\n",
    "print(\"\\n‚úÖ Fine-tuned model evaluation finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# ## 5. Workflow B: Evaluate a Pre-Trained Classifier\n",
    "#\n",
    "# This workflow allows you to skip fine-tuning and directly evaluate a\n",
    "# model that has already been trained for this specific task.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5.1: Download Pre-Trained Classifier ---\n",
    "checkpoint_path = './RETFound_IDRiD_Classifier.pth'\n",
    "if not os.path.isfile(checkpoint_path):\n",
    "    print(\"‚è≥ Model checkpoint not found. Downloading from Google Drive...\")\n",
    "    # Note: Using the gdown ID for the classifier model\n",
    "    !gdown --id 1b0grTwARX1cXnYnMB3ZJZES26aMkgkvZ -O {checkpoint_path}\n",
    "    print(\"‚úÖ Model download complete.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Pre-trained classifier already exists at '{checkpoint_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5.2: Run Inference ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nüöÄ Starting inference on the PRE-TRAINED classifier...\")\n",
    "eval_parser_pretrained = get_args_parser()\n",
    "args_eval_pretrained = eval_parser_pretrained.parse_args([\n",
    "    '--model', 'RETFound_mae',\n",
    "    '--eval',\n",
    "    '--data_path', '../IDRiD_data',\n",
    "    '--resume', checkpoint_path\n",
    "])\n",
    "finetune_main(args_eval_pretrained)\n",
    "print(\"\\n‚úÖ Pre-trained classifier evaluation finished.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
